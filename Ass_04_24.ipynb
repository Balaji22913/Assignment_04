{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3679177a-f1b7-4b55-9a25-527e7d9a0567",
   "metadata": {},
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f11d5-7505-4958-a8d9-0963e8cf731c",
   "metadata": {},
   "source": [
    "Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data. It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2baf80-1583-40bb-ad64-2e44fbfd0f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5100c98-8323-47d6-bfc5-1f9cfc23e055",
   "metadata": {},
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba034b-ec7e-41b8-a682-274146a0b05e",
   "metadata": {},
   "source": [
    "PCA seeks to solve a sequence of optimization problems. The first in the sequence is the unconstrained problem maximizeuTSuuTu,u∈Rp. Since uTu=‖u‖22=‖u‖‖u‖, the above unconstrained problem is equivalent to the constrained problem maximizeuTSusubject touTu=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f53d942-6e8b-4c54-8014-e1e3d35fe312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd9b8a65-9c0d-48d4-b629-d2e7c441055d",
   "metadata": {},
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af08fe-f12a-4308-812e-1f3e36847246",
   "metadata": {},
   "source": [
    "PCA is simply described as “diagonalizing the covariance matrix”. What does diagonalizing a matrix mean in this context? It simply means that we need to find a non-trivial linear combination of our original variables such that the covariance matrix is diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e2538-fc61-4b95-b970-2ce1729f33e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8732ec7-c886-44a5-a92c-365d2cceb0a1",
   "metadata": {},
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad62c63-0dca-43d0-bc24-fbcdd92b8182",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6553284-ef36-4298-af69-04990d6a866b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8438ec-c636-4bde-b139-8c026a3302ab",
   "metadata": {},
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07918c-14f5-4ef5-9af6-7192ce3f5756",
   "metadata": {},
   "source": [
    "The basic idea when using PCA as a tool for feature selection is to select variables according to the magnitude (from largest to smallest in absolute values) of their coefficients (loadings).          \n",
    "Advantages of PCA:            \n",
    "Easy to compute. PCA is based on linear algebra, which is computationally easy to solve by computers.               \n",
    "Speeds up other machine learning algorithms.             \n",
    "Counteracts the issues of high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bad5f1-2122-42c1-adbf-a7aeb8a314c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9f88b73-b068-47c0-a2ba-7cbc6476d61f",
   "metadata": {},
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6ba66-4b28-4dc0-b769-33d7d83ab4c9",
   "metadata": {},
   "source": [
    "PCA is used to visualize multidimensional data.                  \n",
    "It is used to reduce the number of dimensions in healthcare data.               \n",
    "PCA can help resize an image.                 \n",
    "It can be used in finance to analyze stock data and forecast returns.               \n",
    "PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3f5dc-e3c5-4d01-a77d-9dbc69dd7115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92c1237d-d7e1-4eb7-b53a-644ed2de76c8",
   "metadata": {},
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916980c3-49ba-4d29-bc02-67d015ccd0b7",
   "metadata": {},
   "source": [
    "Variance: Variance is the spread of the data in a dataset. In PCA, the variables are transformed in such a way that they explain variance of the dataset in decreasing manner. Co-variance: Covariance provides a measure of the strength of the correlation between two or more sets of random variates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fa80b9-cc79-4050-beaf-5f6d700a187e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b972cc09-2869-4d88-ad5c-f05369891aca",
   "metadata": {},
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b16ae3-1352-4e3a-bd47-9a5e7ad01582",
   "metadata": {},
   "source": [
    "The principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6b03e-70f2-429e-b5a6-9a8be9f54574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308ecc63-6473-4876-a8e8-0b234dea49a3",
   "metadata": {},
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e2847-81dd-452a-a258-e8d430f805c6",
   "metadata": {},
   "source": [
    "PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51efb2b9-efe5-423c-a818-cbf70cdd1b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
