{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb485a37-2ee0-4bdc-a91b-b4f2516d6616",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcfa7aa-7fcd-407f-9f1e-7d1a3b07edf3",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors Algorithm. The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef765b8-ca85-4d1a-a02c-d0b36e27328d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88a4c4dd-6e4f-4961-bb2d-a8bbacfc8791",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51407198-2dfa-462e-8798-0c943c126afc",
   "metadata": {},
   "source": [
    "The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d94e53-a8fc-44b4-af42-5e8344d22fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcf7465f-7b35-4cc4-a2b9-b45aaf8e184a",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d60ee19-08c4-4ead-a77c-ea990dd11509",
   "metadata": {},
   "source": [
    "KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7d844-1845-4e7b-900b-3fa68cf90c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4f0499-195b-431d-ad9b-080d866dfd1f",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b424a522-686d-4cca-9002-fc5ced1f3198",
   "metadata": {},
   "source": [
    "The k-nearest neighbour classification (k-NN) is one of the most popular distance-based algorithms. This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output. The traditional k-NN classifier works naturally with numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750a05dc-2c23-4d6a-9ac7-f53c8d7b6bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1079cf22-4611-44a5-8e9a-072212a3f493",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5c4b2-c7ae-4bed-a44e-9373b0e5db48",
   "metadata": {},
   "source": [
    "The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077453d-6bbd-4a15-a32f-a2508c375b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74b51f75-3428-43c8-af06-4c174510195b",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36768af-2d66-4d0e-b8ba-f32ec3827ea0",
   "metadata": {},
   "source": [
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c1312-d7ea-4725-a8d2-a4a4edf0e59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c87fab6e-175a-4982-be69-ae526cedde94",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ad7e2-4b6d-4365-9125-0d985e62352d",
   "metadata": {},
   "source": [
    "The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\n",
    "KNN classifier is best for classification problems and regressor is best in situations where continuous data is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b30e9d-d4ce-4ca3-b406-46d15d31eac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21211877-f813-4f59-b3a7-b62d45ea7e95",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852429e-dfca-4aae-a3a4-aa815ee97588",
   "metadata": {},
   "source": [
    "Advantages of KNN            \n",
    "\n",
    "1. No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc.              \n",
    "\n",
    "2. Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.                \n",
    "\n",
    "3. KNN is very easy to implement. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)               \n",
    "\n",
    "Disadvantages of KNN             \n",
    "\n",
    "1. Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm.              \n",
    "          \n",
    "2. Does not work well with high dimensions: The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension.                  \n",
    "\n",
    "3. Need feature scaling: We need to do feature scaling (standardization and normalization) before applying KNN algorithm to any dataset. If we don't do so, KNN may generate wrong predictions.              \n",
    "\n",
    "4. Sensitive to noisy data, missing values and outliers: KNN is sensitive to noise in the dataset. We need to manually impute missing values and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ad97e-cdf5-467b-9666-14b25aa707fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "0d11e671-7940-4597-b8db-eee0e8f80616",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f14b89-50e8-42af-88fe-56f9c945bdf8",
   "metadata": {},
   "source": [
    "Manhattan distance captures the distance between two points by aggregating the pairwise absolute difference between each variable while Euclidean distance captures the same by aggregating the squared difference in each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400985ec-2e8d-4f23-aa8f-cbea5ea31998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b10fb052-63a2-4b18-a197-6ca1444046de",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843d1a8-407d-437c-8200-f84e6e5110a2",
   "metadata": {},
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. If not scaled, the feature with a higher value range starts dominating when calculating distances. KNN which uses Euclidean distance is one such algorithm which essentially require scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3869ae1-eb97-432f-b04c-0ccee1d73b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
