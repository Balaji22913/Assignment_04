{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc9a35c-dfa2-48c4-b86c-53537fb8a788",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7689d826-37c8-42da-af69-c60b46013b68",
   "metadata": {},
   "source": [
    "What is boosting in machine learning? Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a45ab-3d60-4e3d-89b3-75a17836fda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26df21ef-4d17-4e66-84bb-15e46ea9957b",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364355f0-7561-4736-9691-7d7c13dd5194",
   "metadata": {},
   "source": [
    "Boosting is a resilient method that curbs over-fitting easily. One disadvantage of boosting is that it is sensitive to outliers since every classifier is obliged to fix the errors in the predecessors. Thus, the method is too dependent on outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3199d8cb-943a-4458-be0b-f06b09d61f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41337580-e064-4ef1-b710-d0f7b8ba85f8",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8018193-f12b-43bc-bac1-25f803ee6854",
   "metadata": {},
   "source": [
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine learning software, called machine learning models, on labeled data to make guesses about unlabeled data. A single machine learning model might make prediction errors depending on the accuracy of the training dataset. For example, if a cat-identifying model has been trained only on images of white cats, it may occasionally misidentify a black cat. Boosting tries to overcome this issue by training multiple models sequentially to improve the accuracy of the overall system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81145f6d-e9d0-42f1-8703-1f876d117f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e2593d-affa-4cda-a23d-539b728302b8",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5644638-6ae5-4eef-a4b8-024883f78819",
   "metadata": {},
   "source": [
    "Adaboost, Gradient boost and Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480d3fc-b7f9-41eb-a60a-03acbd9884c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e74839-7e5d-499a-bf4b-83caee74db0b",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94519027-ca7a-41f3-9784-b83597d7600c",
   "metadata": {},
   "source": [
    "There are many parameters   \n",
    "learning_rate=0.1 (shrinkage).     \n",
    "n_estimators=100 (number of trees).        \n",
    "max_depth=3.     \n",
    "min_samples_split=2.    \n",
    "min_samples_leaf=1.    \n",
    "subsample=1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baca0c0-aa7d-4fb9-9a60-f6d74c06fbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86c600be-acb7-49bd-8e5b-96f9d83fe7a9",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb3cac-744b-4846-b269-ba05dcfa19e2",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentiallyâ€”that is, each model tries to compensate for the weaknesses of its predecessor.In boosting we use weak learners mostly since they are trained faster compared to strong learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15167a6a-0feb-4976-ad53-54f07f9ff02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acea939a-fe02-4e01-a3a6-4f591da14421",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5453c-7099-4852-92fa-2a92f1d83532",
   "metadata": {},
   "source": [
    "AdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method. The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. These trees are also called Decision Stumps.     \n",
    "An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe10a3-223e-40fd-a2eb-f6f1c430b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4417eb-c288-42ec-b0af-b0fbb4fef725",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af84f6-210e-454c-a33a-c3f555bb3651",
   "metadata": {},
   "source": [
    "The error function that AdaBoost uses is an exponential loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0321990-e4c0-44c1-a8a6-e286ed9d157e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f49fce00-c6b2-4c46-bb99-a290fd0a803e",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986435c2-6b56-4453-81a2-e05e7f704ce3",
   "metadata": {},
   "source": [
    "In the boosting algorithm,AdaBoost ,those observations which were misclassified by the classifier in the (m-1)th step have their weights increased in the mth step, and those which were correctly classified have their weights decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfd9b3-da2c-4eaa-bf35-3ef9ce58a500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e71c1455-5d98-45ef-98a5-57c3e8ff53fe",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403b128-9044-4a7b-982c-24f0f5d73da6",
   "metadata": {},
   "source": [
    "More trees usually means higher accuracy at the cost of slower learning. If you wish to speed up your random forest, lower the number of estimators. If you want to increase the accuracy of your model, increase the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c06e3-6276-4fdd-901e-1780717aff4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
