{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff0f3d5-9d91-40d9-b59d-9db03a0a441e",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257d3fd-b731-4aec-9c1c-67ffafb9938d",
   "metadata": {},
   "source": [
    "By combining multiple models, bagging helps reduce the model's variance and can prevent overfitting by introducing diversity into the training process. It is commonly used with decision trees but can also be applied to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f0ea6-83ed-4bc0-b54e-f471cf91df08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd5c9699-9d4a-4fc3-95a4-07637ecc2dfe",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a52b13-6f09-4cd4-80a6-0463dbdf4e1f",
   "metadata": {},
   "source": [
    "Bagging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner. It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure. One disadvantage of bagging is that it introduces a loss of interpretability of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6da73-75aa-420c-b32d-a8dc4b91b168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc7f63d-85ee-4b4a-8edb-4c0ade052b6a",
   "metadata": {},
   "source": [
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9c8e6-cc8b-4b9e-b59d-c2257fdd21ae",
   "metadata": {},
   "source": [
    "If the model is very simple with fewer parameters, it may have low variance and high bias. Whereas, if the model has a large number of parameters, it will have high variance and low bias.When we are creating a model, we want to strike a balance between bias and variance. Bias and Variance are opposite of each other so whenever we try to reduce the variance, we are increasing the bias of the model at the same time. This dilemma of overfitting/underfitting is called Bias-Variance Tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b728a3-6672-415f-8f3e-fb543337bf49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ade84e6f-c108-48f6-9f9b-1eb3c915e808",
   "metadata": {},
   "source": [
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06957512-b6f1-4209-8463-9119dc4169ba",
   "metadata": {},
   "source": [
    "Yes, in case of classification we take maximum voting as final result and in case of regression we take mean of the outputs of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e92dd-48d3-4c39-b3f1-6327b8297535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5132ccdf-a8b6-41dc-b1fa-484d5bd104a7",
   "metadata": {},
   "source": [
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58642904-96f4-479c-810a-14bebdcb3cfd",
   "metadata": {},
   "source": [
    "Bagging is a powerful ensemble method which helps to reduce variance, and by extension, prevent overfitting. Ensemble methods improve model precision by using a group (or \"ensemble\") of models which, when combined, outperform individual models when used separately.\n",
    "There are no restrictions/guidelines on the number of models. You can start even from 3 models. You can keep the number of models as a hyperparameter if the training cost is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef6d57-e3b4-485c-9107-647cd51bd55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e22f2061-bb3d-4dc5-8d48-0d6262777e06",
   "metadata": {},
   "source": [
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe322fa-efdb-43da-8c07-573910469d49",
   "metadata": {},
   "source": [
    " an example of the bagging technique is the random forest algorithm. The random forest is an ensemble of multiple decision trees. Decision trees tend to be prone to overfitting. Because of this, a single decision tree can't be relied on for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cc31a-cbe6-4860-b65f-dbac9f9ca206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
